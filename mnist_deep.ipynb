{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-429a0d4878c3>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Jaeil\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Jaeil\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Jaeil\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Jaeil\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jaeil\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jaeil\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#MNIST 손글씨 이미지 데이터 읽어들이기\n",
    "mnist = input_data.read_data_sets(\"mnist/\", one_hot=True)\n",
    "\n",
    "pixels = 28*28 #28x28 픽셀\n",
    "nums = 10 #0-9사이의 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#플레이스홀더 정의하기\n",
    "x = tf.placeholder(tf.float32, shape = (None, pixels), name=\"x\") #이미지 데이터\n",
    "y_ = tf.placeholder(tf.float32, shape = (None, nums), name=\"y_\") #정답 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치와 바이어스를 초기화하는 함수\n",
    "def weight_variable(name, shape):\n",
    "    W_init = tf.truncated_normal(shape, stddev=0.1)\n",
    "    W = tf.Variable(W_init, name=\"W_\"+name)\n",
    "    return W\n",
    "\n",
    "def bias_variable(name, size):\n",
    "    b_init = tf.constant(0.1, shape=[size])\n",
    "    b= tf.Variable(b_init, name=\"b_\"+name)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성곱 계층을 만드는 함수\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최대 풀링층을 만드는 함수\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성곱층1\n",
    "with tf.name_scope('conv1')as scope:\n",
    "    W_conv1 = weight_variable('conv1', [5,5,1,32])\n",
    "    b_conv1 = bias_variable('conv1', 32)\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#풀링층1\n",
    "with tf.name_scope('pool1')as scope:\n",
    "    h_pool1 = max_pool(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성곱층2\n",
    "with tf.name_scope('conv2')as scope:\n",
    "    W_conv2 = weight_variable('conv2', [5,5,32,64])\n",
    "    b_conv2 = bias_variable('conv2', 64)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#풀링층2\n",
    "with tf.name_scope('pool2')as scope:\n",
    "    h_pool2 = max_pool(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전결합층\n",
    "with tf.name_scope('fully_connected')as scope:\n",
    "    n = 7*7*64\n",
    "    W_fc = weight_variable('fc', [n, 1024])\n",
    "    b_fc = bias_variable('fc', 1024)\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1,n])\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc)+b_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-5994a86672d9>:4: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#드롭아웃(오버피팅) 막기\n",
    "with tf.name_scope('dropout')as scope:\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc_drop = tf.nn.dropout(h_fc, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#출력층\n",
    "with tf.name_scope('readout')as scope:\n",
    "    W_fc2 = weight_variable('fc2', [1024,10])\n",
    "    b_fc2 = bias_variable('fc2', 10)\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc_drop, W_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습시키기\n",
    "with tf.name_scope('loss')as scope:\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "with tf.name_scope('training')as scope:\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 평가하기\n",
    "with tf.name_scope('predict')as scope:\n",
    "    predict_step = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_,1))\n",
    "    accuracy_step = tf.reduce_mean(tf.cast(predict_step, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed_dict 설정하기\n",
    "def set_feed(images, labels, prob):\n",
    "    return {x: images, y_ : labels, keep_prob:prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0 loss= 477.4867 acc= 0.1711\n",
      "step= 100 loss= 49.622566 acc= 0.849\n",
      "step= 200 loss= 45.811306 acc= 0.9152\n",
      "step= 300 loss= 23.477467 acc= 0.9284\n",
      "step= 400 loss= 11.511037 acc= 0.9394\n",
      "step= 500 loss= 11.193548 acc= 0.9465\n",
      "step= 600 loss= 14.064208 acc= 0.9512\n",
      "step= 700 loss= 9.548792 acc= 0.9569\n",
      "step= 800 loss= 14.98844 acc= 0.9588\n",
      "step= 900 loss= 8.564086 acc= 0.9642\n",
      "step= 1000 loss= 19.689133 acc= 0.9609\n",
      "step= 1100 loss= 5.4488425 acc= 0.9663\n",
      "step= 1200 loss= 1.8088338 acc= 0.969\n",
      "step= 1300 loss= 12.199297 acc= 0.9713\n",
      "step= 1400 loss= 2.3286042 acc= 0.9732\n",
      "step= 1500 loss= 5.8512526 acc= 0.9722\n",
      "step= 1600 loss= 3.3809063 acc= 0.9747\n",
      "step= 1700 loss= 3.142814 acc= 0.9738\n",
      "step= 1800 loss= 3.3827357 acc= 0.9765\n",
      "step= 1900 loss= 6.416314 acc= 0.9749\n",
      "step= 2000 loss= 2.1367373 acc= 0.978\n",
      "step= 2100 loss= 2.334475 acc= 0.9768\n",
      "step= 2200 loss= 1.7006538 acc= 0.9769\n",
      "step= 2300 loss= 2.1535819 acc= 0.9803\n",
      "step= 2400 loss= 5.64052 acc= 0.9794\n",
      "step= 2500 loss= 4.7208233 acc= 0.9783\n",
      "step= 2600 loss= 3.026155 acc= 0.9808\n",
      "step= 2700 loss= 9.129506 acc= 0.9801\n",
      "step= 2800 loss= 19.76505 acc= 0.9812\n",
      "step= 2900 loss= 2.0282843 acc= 0.9816\n",
      "step= 3000 loss= 0.9951815 acc= 0.9798\n",
      "step= 3100 loss= 12.678278 acc= 0.9819\n",
      "step= 3200 loss= 0.71604913 acc= 0.9819\n",
      "step= 3300 loss= 3.3004255 acc= 0.9846\n",
      "step= 3400 loss= 3.6864834 acc= 0.9838\n",
      "step= 3500 loss= 0.6777216 acc= 0.9844\n",
      "step= 3600 loss= 8.499482 acc= 0.9834\n",
      "step= 3700 loss= 7.048321 acc= 0.9863\n",
      "step= 3800 loss= 4.553229 acc= 0.9847\n",
      "step= 3900 loss= 4.8660955 acc= 0.9832\n",
      "step= 4000 loss= 0.8693876 acc= 0.9836\n",
      "step= 4100 loss= 3.3910534 acc= 0.9858\n",
      "step= 4200 loss= 10.464743 acc= 0.9858\n",
      "step= 4300 loss= 2.6817245 acc= 0.9854\n",
      "step= 4400 loss= 1.6817341 acc= 0.9855\n",
      "step= 4500 loss= 2.0156248 acc= 0.9844\n",
      "step= 4600 loss= 14.45498 acc= 0.9865\n",
      "step= 4700 loss= 3.3632207 acc= 0.9879\n",
      "step= 4800 loss= 5.4408092 acc= 0.9871\n",
      "step= 4900 loss= 0.96750164 acc= 0.9874\n",
      "step= 5000 loss= 3.199948 acc= 0.9862\n",
      "step= 5100 loss= 3.5196667 acc= 0.9881\n",
      "step= 5200 loss= 2.7557297 acc= 0.9867\n",
      "step= 5300 loss= 0.28745273 acc= 0.9877\n",
      "step= 5400 loss= 3.5587478 acc= 0.9881\n",
      "step= 5500 loss= 0.3004732 acc= 0.9873\n",
      "step= 5600 loss= 1.0838451 acc= 0.9883\n",
      "step= 5700 loss= 1.585502 acc= 0.9871\n",
      "step= 5800 loss= 0.8380572 acc= 0.9869\n",
      "step= 5900 loss= 0.7244343 acc= 0.9884\n",
      "step= 6000 loss= 3.1983993 acc= 0.9879\n",
      "step= 6100 loss= 0.04734308 acc= 0.9872\n",
      "step= 6200 loss= 6.785919 acc= 0.9878\n",
      "step= 6300 loss= 3.6457586 acc= 0.9871\n",
      "step= 6400 loss= 1.1608117 acc= 0.9882\n",
      "step= 6500 loss= 0.17867616 acc= 0.9877\n",
      "step= 6600 loss= 2.0390606 acc= 0.9893\n",
      "step= 6700 loss= 0.16658568 acc= 0.9894\n",
      "step= 6800 loss= 0.2336783 acc= 0.9883\n",
      "step= 6900 loss= 1.3295957 acc= 0.9895\n",
      "step= 7000 loss= 1.5126839 acc= 0.988\n",
      "step= 7100 loss= 0.6769315 acc= 0.9892\n",
      "step= 7200 loss= 0.7097553 acc= 0.9886\n",
      "step= 7300 loss= 0.41822377 acc= 0.9895\n",
      "step= 7400 loss= 0.116183564 acc= 0.9889\n",
      "step= 7500 loss= 0.41005635 acc= 0.9901\n",
      "step= 7600 loss= 0.8434346 acc= 0.9871\n",
      "step= 7700 loss= 0.4794432 acc= 0.9892\n",
      "step= 7800 loss= 0.5802778 acc= 0.9881\n",
      "step= 7900 loss= 16.36758 acc= 0.988\n",
      "step= 8000 loss= 0.3853637 acc= 0.9894\n",
      "step= 8100 loss= 3.6576333 acc= 0.987\n",
      "step= 8200 loss= 0.4235749 acc= 0.9898\n",
      "step= 8300 loss= 0.7226171 acc= 0.9875\n",
      "step= 8400 loss= 0.64900124 acc= 0.989\n",
      "step= 8500 loss= 2.3041127 acc= 0.9897\n",
      "step= 8600 loss= 6.6069293 acc= 0.9893\n",
      "step= 8700 loss= 0.13387628 acc= 0.9889\n",
      "step= 8800 loss= 0.5499528 acc= 0.9895\n",
      "step= 8900 loss= 0.98043513 acc= 0.991\n",
      "step= 9000 loss= 0.079801776 acc= 0.9892\n",
      "step= 9100 loss= 4.2427263 acc= 0.9894\n",
      "step= 9200 loss= 0.09266603 acc= 0.9901\n",
      "step= 9300 loss= 0.35063782 acc= 0.9907\n",
      "step= 9400 loss= 3.9102633 acc= 0.9904\n",
      "step= 9500 loss= 0.1682153 acc= 0.9885\n",
      "step= 9600 loss= 1.3637326 acc= 0.9886\n",
      "step= 9700 loss= 0.47112536 acc= 0.9906\n",
      "step= 9800 loss= 0.58419514 acc= 0.9892\n",
      "step= 9900 loss= 2.466054 acc= 0.9887\n",
      "정답률 = 0.9912\n"
     ]
    }
   ],
   "source": [
    "#세션 시작하기\n",
    "with tf.Session()as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #TensorBoard 준비하기\n",
    "    tw = tf.summary.FileWriter('log_dir', graph=sess.graph)\n",
    "    #테스트 전용 피드 만들기\n",
    "    test_fd = set_feed(mnist.test.images, mnist.test.labels, 1)\n",
    "    #학습 시작하기\n",
    "    for step in range(10000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        fd = set_feed(batch[0], batch[1], 0.5)\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict=fd)\n",
    "        if step % 100 ==0:\n",
    "            acc = sess.run(accuracy_step, feed_dict=test_fd)\n",
    "            print(\"step=\", step, \"loss=\", loss, \"acc=\", acc)\n",
    "     #최종적인 결과 출력하기\n",
    "    acc=sess.run(accuracy_step, feed_dict=test_fd)\n",
    "    print(\"정답률 =\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
